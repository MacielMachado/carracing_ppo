# -*- coding: utf-8 -*-
"""bc_carracing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16Z4-FksSFjdFbQWtqQi75l2PGOfPy91o
"""

from pathlib import Path

import torch
import torch.nn as nn

import pandas as pd

import torch.optim as optim
from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler

from gym.envs.box2d import CarRacing
from gym.wrappers.frame_stack import FrameStack
from gym.wrappers import TimeLimit
from gym.wrappers import RecordVideo

from env import EpisodeReward, TransformAction, TransformImage
from policy import Policy


def always_true(i_epoch):
    return True

video_path = "./video_expert"
env_video = CarRacing(render_mode="rgb_array", domain_randomize=False)
env_video = FrameStack(env_video, 4)
env_video = TransformImage(env_video)
env_video = TimeLimit(env_video, 1000)
env_video = TransformAction(env_video)
env_video = RecordVideo(env_video, video_path, episode_trigger=always_true)

env_video.reset()

actor_critic = Policy(
    env_video.observation_space.shape,
    env_video.action_space,
    activation=None
)
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
actor_critic = actor_critic.to(device)

saved_variables = torch.load('model_136.pt', map_location='cuda')
actor_critic.load_state_dict(saved_variables)

obs, _ = env_video.reset()
done = False
step = 0
while not done:
  obs = torch.from_numpy(obs).float()
  obs = obs.unsqueeze(0).to(device)
  with torch.no_grad():
      _, action, _ = actor_critic.act(obs, deterministic=True)
  action = action[0].cpu().numpy()
  obs, reward, done, truncated, info = env_video.step(action)
  done |= truncated
  step += 1

env = CarRacing(render_mode="rgb_array", domain_randomize=False)
env = FrameStack(env, 4)
env = TransformImage(env)
env = TimeLimit(env, 1000)
env = TransformAction(env)

actions = []
observations = []

n_episodes = 10
for i_epoch in range(n_episodes):
    print('i_epoch: ', i_epoch)
    obs, _ = env.reset()
    done = False
    step = 0
    while not done:
      obs = torch.from_numpy(obs).float()
      obs = obs.unsqueeze(0).to(device)
      with torch.no_grad():
          _, action, _ = actor_critic.act(obs, deterministic=False)
      action = action[0].cpu().numpy()
      obs, reward, done, truncated, info = env.step(action)
      done |= truncated
      observations.append(obs)
      actions.append(action)
      step += 1

expert_state = torch.Tensor(observations)
expert_actions = torch.Tensor(actions)

actions = []
observations = []

n_episodes = 2
for i_epoch in range(n_episodes):
    print('i_epoch: ', i_epoch)
    obs, _ = env.reset()
    done = False
    step = 0
    while not done:
      obs = torch.from_numpy(obs).float()
      obs = obs.unsqueeze(0).to(device)
      with torch.no_grad():
          _, action, _ = actor_critic.act(obs, deterministic=False)
      action = action[0].cpu().numpy()
      obs, reward, done, truncated, info = env.step(action)
      done |= truncated
      observations.append(obs)
      actions.append(action)
      step += 1

eval_state = torch.Tensor(observations)
eval_actions = torch.Tensor(actions)

bc_actor = Policy(
    env_video.observation_space.shape,
    env_video.action_space,
    activation=None
)
bc_actor = bc_actor.to(device)
optimizer = optim.Adam(bc_actor.parameters(), lr=1e-3, eps=1e-5)
n_epochs = 200
max_grad_norm = 0.5
losses = []
entropies = []
losses_bc_samples = []
eval_losses = []
eval_entropies = []
mini_batch_size = 256
sampler = BatchSampler(
            SubsetRandomSampler(range(expert_actions.shape[0])),
            mini_batch_size,
            drop_last=True)
eval_sampler = BatchSampler(
            SubsetRandomSampler(range(eval_actions.shape[0])),
            mini_batch_size,
            drop_last=True)
ep_rewards = []
ep_rewards_samples = []
bc_samples = 0
ent_weight = 0.01
ckpt_dir = Path('ckpt/')
ckpt_dir.mkdir(exist_ok=True)
for i_epoch in range(n_epochs):
    total_loss = 0
    total_entropy = 0
    n_total = 0
    for indices in sampler:
        expert_state_batch = expert_state[indices]
        expert_actions_batch = expert_actions[indices]
        expert_state_batch = expert_state_batch.to(device)
        expert_actions_batch = expert_actions_batch.to(device)
        values, actions_log_probs, entropy_loss = bc_actor.evaluate_actions(expert_state_batch, expert_actions_batch)
        bc_loss = -actions_log_probs.mean()
        loss = bc_loss +  ent_weight * entropy_loss
        optimizer.zero_grad()
        loss.backward()
        nn.utils.clip_grad_norm_(bc_actor.parameters(),
                                  max_grad_norm)
        optimizer.step()
        total_loss += loss.item()
        total_entropy += entropy_loss.item()
        n_total += expert_state_batch.shape[0]
    bc_samples += n_total
    losses.append(total_loss / n_total)
    losses_bc_samples.append(bc_samples)
    print('bc_loss: ', total_loss / n_total)
    entropies.append(total_entropy / n_total)
    print('bc_entropy: ', total_entropy / n_total)

    eval_loss = 0
    eval_entropy = 0
    eval_n_total = 0
    for indices in eval_sampler:
        eval_state_batch = eval_state[indices]
        eval_actions_batch = eval_actions[indices]
        eval_state_batch = eval_state_batch.to(device)
        eval_actions_batch = eval_actions_batch.to(device)
        with torch.no_grad():
            values, actions_log_probs, entropy_loss = bc_actor.evaluate_actions(eval_state_batch, eval_actions_batch)
        bc_loss = -actions_log_probs.mean()
        loss = bc_loss +  ent_weight * entropy_loss
        eval_loss += loss.item()
        eval_entropy += entropy_loss.item()
        eval_n_total += eval_state_batch.shape[0]
    eval_losses.append(eval_loss / eval_n_total)
    print('eval_loss: ', eval_loss / eval_n_total)
    eval_entropies.append(eval_entropy / eval_n_total)
    print('eval_entropy: ', eval_entropy / eval_n_total)

    if i_epoch % 5 == 0:
        torch.save(bc_actor.state_dict(), ckpt_dir / 'model_{}.pt'.format(i_epoch))
        obs, _ = env.reset()
        done = False
        step = 0
        ep_reward = 0
        while not done:
            obs = torch.from_numpy(obs).float()
            obs = obs.unsqueeze(0).to(device)
            with torch.no_grad():
                _, action, _ = bc_actor.act(obs, deterministic=True)
            action = action[0].cpu().numpy()
            obs, reward, done, truncated, info = env.step(action)
            done |= truncated
            ep_reward += reward
            observations.append(obs)
            actions.append(action)
            step += 1
        ep_rewards.append(ep_reward)
        ep_rewards_samples.append(bc_samples)
        print('Eval agent: ', bc_samples, 'samples, ep reward: ', ep_reward)

loss_df = pd.DataFrame(data={'loss': losses, 'bc_samples': losses_bc_samples})
loss_df.to_csv('loss.csv')

entropy_df = pd.DataFrame(data={'entropies': entropies, 'bc_samples': losses_bc_samples})
entropy_df.to_csv('entropy.csv')

ep_rewards_df = pd.DataFrame(data={'ep_rewards': ep_rewards, 'ep_rewards_samples': ep_rewards_samples})
ep_rewards_df.to_csv('ep_rewards.csv')

eval_loss_df = pd.DataFrame(data={'loss': eval_losses, 'bc_samples': losses_bc_samples})
eval_loss_df.to_csv('eval_loss.csv')

eval_entropy_df = pd.DataFrame(data={'entropies': eval_entropies, 'bc_samples': losses_bc_samples})
eval_entropy_df.to_csv('eval_entropy.csv')