# -*- coding: utf-8 -*-
"""bc_car-racing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ST87ylTTE2ml3JxT-X0dUrVW1T8JqNDT
"""

# !pip install 'shimmy>=0.2.1'

# !pip install -q swig
# !pip install -q gymnasium[box2d]
# !pip install stable_baselines3

import gymnasium as gym
import torch
import matplotlib.pyplot as plt
import numpy as np

class TransformImage(gym.ObservationWrapper):
    def __init__(
        self,
        env: gym.Env,
    ):
        gym.ObservationWrapper.__init__(self, env)

        self.observation_space = gym.spaces.Box(
            low=0, high=255, shape=(env.observation_space.shape[0]*env.observation_space.shape[3], env.observation_space.shape[1], env.observation_space.shape[2]), dtype=np.uint8
        )

    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        obs = np.transpose(obs, (0, 3, 1, 2))
        obs = np.reshape(obs, (obs.shape[0]*obs.shape[1], obs.shape[2], obs.shape[3]))
        return obs, reward, terminated, truncated, info

    def reset(self, **kwargs):
        kwargs['seed'] = 45
        obs, info = self.env.reset(**kwargs)
        obs = np.transpose(obs, (0, 3, 1, 2))
        obs = np.reshape(obs, (obs.shape[0]*obs.shape[1], obs.shape[2], obs.shape[3]))
        return obs, info

class TransformAction(gym.ActionWrapper):
    def __init__(
        self,
        env: gym.Env,
    ):
        gym.ActionWrapper.__init__(self, env)

        self.action_space = gym.spaces.Box(
            np.array([-1, -1]).astype(np.float32),
            np.array([+1, +1]).astype(np.float32),
        )

    def action(self, action):
        steer, acc = action.astype(np.float64)
        if acc >= 0.0:
            gas = acc
            brake = 0.0
        else:
            gas = 0.0
            brake = np.abs(acc)
        action = np.array([steer, gas, brake])

        return action

class EpisodeReward(gym.ObservationWrapper):
    def __init__(
        self,
        env: gym.Env,
    ):
        gym.ObservationWrapper.__init__(self, env)
        self.episode_reward = 0

    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        self.episode_reward += reward
        if terminated or truncated:
            info['episode'] = {'r': self.episode_reward}
        return obs, reward, terminated, truncated, info

    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        self.episode_reward = 0
        return obs, info

from gym.envs.box2d import CarRacing
from gym.wrappers.frame_stack import FrameStack
from gym.wrappers.normalize import NormalizeObservation, NormalizeReward
from gym.wrappers import TimeLimit, TransformObservation, AutoResetWrapper


def make_env():
    env = CarRacing(render_mode="rgb_array", domain_randomize=False)
    env = TimeLimit(env, 1000)
    env = AutoResetWrapper(env)  # Reinicializa o ambiente quando este chega em done
    env = FrameStack(env, 4)
    env = TransformImage(env)  # Reestrutura a ordem das observations e o reset, alem disso mudo o observation_space
    env = EpisodeReward(env)
    env = TransformAction(env)  # Muda a ação para ter apenas duas dimensoes
    return env

from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecNormalize

import torch.nn as nn

class AddBias(nn.Module):
    def __init__(self, bias):
        super(AddBias, self).__init__()
        self._bias = nn.Parameter(bias.unsqueeze(1))

    def forward(self, x):
        if x.dim() == 2:
            bias = self._bias.t().view(1, -1)
        else:
            bias = self._bias.t().view(1, -1, 1, 1)

        return x + bias

def init(module, weight_init, bias_init, gain=1):
    weight_init(module.weight.data, gain=gain)
    bias_init(module.bias.data)
    return module

import math

import torch
import torch.nn as nn
import torch.nn.functional as F

"""
Modify standard PyTorch distributions so they are compatible with this code.
"""

#
# Standardize distribution interfaces
#

# Categorical

# Normal
class FixedNormal(torch.distributions.Normal):
    def log_probs(self, actions):
        return super().log_prob(actions).sum(-1, keepdim=True)

    def entrop(self):
        return super.entropy().sum(-1)

    def mode(self):
        return self.mean


class DiagGaussian(nn.Module):
    def __init__(self, num_inputs, num_outputs, activation=None):
        super(DiagGaussian, self).__init__()

        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.
                               constant_(x, 0))  # é definida para inicializar os pesos da camada linear (nn.Linear) usando a inicialização ortogonal e, em seguida, definir a função de ativação para zero.

        self.fc_mean = init_(nn.Linear(num_inputs, num_outputs))
        self.logstd = AddBias(torch.zeros(num_outputs))
        self.activation = activation

    def forward(self, x):
        action_mean = self.fc_mean(x)
        if self.activation is not None:
            action_mean = self.activation(action_mean)

        #  An ugly hack for my KFAC implementation.
        zeros = torch.zeros(action_mean.size())
        if x.is_cuda:
            zeros = zeros.cuda()

        action_logstd = self.logstd(zeros)
        return FixedNormal(action_mean, action_logstd.exp())

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

class Flatten(nn.Module):
    def forward(self, x):
        return x.view(x.size(0), -1)


class Policy(nn.Module):
    def __init__(self, obs_shape, action_space, activation=None):
        super(Policy, self).__init__()

        self.base = CNNBase(obs_shape[0])

        num_outputs = action_space.shape[0]
        self.dist = DiagGaussian(self.base.output_size, num_outputs, activation=activation)

    def forward(self, inputs):
        raise NotImplementedError

    def act(self, inputs, deterministic=False):
        value, actor_features = self.base(inputs)
        dist = self.dist(actor_features)

        if deterministic:
            action = dist.mode()
        else:
            action = dist.sample()

        action_log_probs = dist.log_probs(action)
        dist_entropy = dist.entropy().mean()

        return value, action, action_log_probs

    def get_value(self, inputs):
        value, _ = self.base(inputs)
        return value

    def evaluate_actions(self, inputs, action):
        value, actor_features = self.base(inputs)
        dist = self.dist(actor_features)

        action_log_probs = dist.log_probs(action)
        dist_entropy = dist.entropy().mean()

        return value, action_log_probs, dist_entropy


class NNBase(nn.Module):
    def __init__(self, hidden_size):
        super(NNBase, self).__init__()

        self._hidden_size = hidden_size

    @property
    def output_size(self):
        return self._hidden_size


class CNNBase(NNBase):
    def __init__(self, num_inputs, hidden_size=512, env=None):
        super(CNNBase, self).__init__(hidden_size)

        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.
                               constant_(x, 0), nn.init.calculate_gain('relu'))

        # For Car Racing
        print("Using CarRacing base")
        self.main = nn.Sequential(
            init_(nn.Conv2d(num_inputs, 32, 4, stride=2)), nn.ReLU(),
            init_(nn.Conv2d(32, 64, 4, stride=2)), nn.ReLU(),
            init_(nn.Conv2d(64, 128, 4, stride=2)), nn.ReLU(),
            init_(nn.Conv2d(128, 256, 4, stride=2)), nn.ReLU(), nn.Flatten()
        )
        self.main2 = nn.Sequential(
            init_(nn.Linear(256 * 4 * 4, hidden_size)), nn.ReLU(),
            init_(nn.Linear(hidden_size, hidden_size)), nn.ReLU(),
        )

        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.
                               constant_(x, 0))

        self.critic_linear = init_(nn.Linear(hidden_size, 1))
        self.train()

    def forward(self, inputs):
        inputs = inputs / 255.0
        # inputs = inputs.permute(0, 1, 4, 2, 3)
        # inputs = inputs.reshape(inputs.size(0), inputs.size(1) * inputs.size(2), inputs.size(3), inputs.size(4))
        x = self.main(inputs)
        x = self.main2(x)

        return self.critic_linear(x), x

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.autograd import Variable

import gym


class PPO():
    def __init__(self,
                 actor_critic,
                 clip_param,
                 ppo_epoch,
                 mini_batch_size,
                 value_loss_coef,
                 entropy_coef,
                 lr=None,
                 eps=None,
                 max_grad_norm=None,
                 use_clipped_value_loss=True,
                 gamma=None,
                 decay=None,
                 act_space=None,
                 ):

        self.actor_critic = actor_critic

        self.clip_param = clip_param
        self.ppo_epoch = ppo_epoch
        self.mini_batch_size = mini_batch_size
        self.act_space = act_space
        print(self.act_space)

        self.value_loss_coef = value_loss_coef
        self.entropy_coef = entropy_coef

        self.gamma = gamma
        self.decay = decay

        self.max_grad_norm = max_grad_norm
        self.use_clipped_value_loss = use_clipped_value_loss

        self.optimizer = optim.Adam(actor_critic.parameters(), lr=lr, eps=eps)

    def update_bc(self, expert_state, expert_actions, obfilt=None):
        if obfilt:
            expert_state = obfilt(expert_state.cpu().numpy(), update=False)
            expert_state = torch.FloatTensor(expert_state).to(expert_actions.device)
            expert_state = Variable(expert_state)
        if isinstance(self.act_space, gym.spaces.Discrete):
            _expert_actions = torch.argmax(expert_actions, 1)
        else:
            _expert_actions = expert_actions
        values, actions_log_probs, _ = self.actor_critic.evaluate_actions(expert_state, _expert_actions)
        loss = -actions_log_probs.mean()
        self.optimizer.zero_grad()
        loss.backward()
        nn.utils.clip_grad_norm_(self.actor_critic.parameters(),
                                 self.max_grad_norm)
        self.optimizer.step()
        return loss

    def get_action_loss(self, expert_state, expert_actions):
        if isinstance(self.act_space, gym.spaces.Discrete):
            _expert_actions = torch.argmax(expert_actions, 1)
        else:
            _expert_actions = expert_actions
        values, actions_log_probs, _ = self.actor_critic.evaluate_actions(expert_state, _expert_actions)
        loss = -actions_log_probs.mean()
        return loss


    def update(self, rollouts, expert_dataset=None, obfilt=None):
        # Expert dataset in case the BC update is required
        advantages = rollouts.returns[:-1] - rollouts.value_preds[:-1]
        advantages = (advantages - advantages.mean()) / (
            advantages.std() + 1e-5)

        value_loss_epoch = 0
        action_loss_epoch = 0
        dist_entropy_epoch = 0
        num_updates = 0

        for e in range(self.ppo_epoch):
            data_generator = rollouts.feed_forward_generator(
                    advantages, mini_batch_size=self.mini_batch_size)

            for sample in data_generator:
                obs_batch, actions_batch, \
                   value_preds_batch, return_batch, masks_batch, old_action_log_probs_batch, \
                        adv_targ = sample

                # Reshape to do in a single forward pass for all steps
                values, action_log_probs, dist_entropy = self.actor_critic.evaluate_actions(obs_batch, actions_batch)

                ratio = torch.exp(action_log_probs -
                                  old_action_log_probs_batch)
                surr1 = ratio * adv_targ
                surr2 = torch.clamp(ratio, 1.0 - self.clip_param,
                                    1.0 + self.clip_param) * adv_targ
                action_loss = -torch.min(surr1, surr2).mean()

                # Expert dataset
                if expert_dataset:
                    for exp_state, exp_action in expert_dataset:
                        if obfilt:
                            exp_state = obfilt(exp_state.numpy(), update=False)
                            exp_state = torch.FloatTensor(exp_state)
                        exp_state = Variable(exp_state).to(action_loss.device)
                        exp_action = Variable(exp_action).to(action_loss.device)
                        # Get BC loss
                        if isinstance(self.act_space, gym.spaces.Discrete):
                            _exp_action = torch.argmax(exp_action, 1)
                        else:
                            _exp_action = exp_action
                        _, alogprobs, _ = self.actor_critic.evaluate_actions(exp_state, _exp_action)
                        bcloss = -alogprobs.mean()
                        # action loss is weighted sum
                        action_loss = self.gamma * bcloss + (1 - self.gamma) * action_loss
                        # Multiply this coeff with decay factor
                        break

                if self.use_clipped_value_loss:
                    value_pred_clipped = value_preds_batch + \
                        (values - value_preds_batch).clamp(-self.clip_param, self.clip_param)
                    value_losses = (values - return_batch).pow(2)
                    value_losses_clipped = (
                        value_pred_clipped - return_batch).pow(2)
                    value_loss = 0.5 * torch.max(value_losses,
                                                 value_losses_clipped).mean()
                else:
                    value_loss = 0.5 * (return_batch - values).pow(2).mean()

                self.optimizer.zero_grad()
                (value_loss * self.value_loss_coef + action_loss -
                 dist_entropy * self.entropy_coef).backward()
                nn.utils.clip_grad_norm_(self.actor_critic.parameters(),
                                         self.max_grad_norm)
                self.optimizer.step()

                value_loss_epoch += value_loss.item()
                action_loss_epoch += action_loss.item()
                dist_entropy_epoch += dist_entropy.item()
                num_updates += 1

        value_loss_epoch /= num_updates
        action_loss_epoch /= num_updates
        dist_entropy_epoch /= num_updates

        if self.gamma is not None:
            self.gamma *= self.decay
            print("gamma {}".format(self.gamma))

        return value_loss_epoch, action_loss_epoch, dist_entropy_epoch


import torch
from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler


def _flatten_helper(T, N, _tensor):
    return _tensor.view(T * N, *_tensor.size()[2:])


class RolloutStorage(object):
    def __init__(self, num_steps, num_processes, obs_shape, action_space):
        self.obs = torch.zeros(num_steps + 1, num_processes, *obs_shape)
        self.rewards = torch.zeros(num_steps, num_processes, 1)
        self.value_preds = torch.zeros(num_steps + 1, num_processes, 1)
        self.returns = torch.zeros(num_steps + 1, num_processes, 1)
        self.action_log_probs = torch.zeros(num_steps, num_processes, 1)
        if action_space.__class__.__name__ == 'Discrete':
            action_shape = 1
        else:
            action_shape = action_space.shape[0]
        self.actions = torch.zeros(num_steps, num_processes, action_shape)
        if action_space.__class__.__name__ == 'Discrete':
            self.actions = self.actions.long()
        self.masks = torch.ones(num_steps + 1, num_processes, 1)

        # Masks that indicate whether it's a true terminal state
        # or time limit end state
        self.bad_masks = torch.ones(num_steps + 1, num_processes, 1)

        self.num_steps = num_steps
        self.step = 0

    def to(self, device):
        self.obs = self.obs.to(device)
        self.rewards = self.rewards.to(device)
        self.value_preds = self.value_preds.to(device)
        self.returns = self.returns.to(device)
        self.action_log_probs = self.action_log_probs.to(device)
        self.actions = self.actions.to(device)
        self.masks = self.masks.to(device)
        self.bad_masks = self.bad_masks.to(device)

    def insert(self, obs, actions, action_log_probs,
               value_preds, rewards, masks, bad_masks):
        self.obs[self.step + 1].copy_(obs)
        self.actions[self.step].copy_(actions)
        self.action_log_probs[self.step].copy_(action_log_probs)
        self.value_preds[self.step].copy_(value_preds)
        self.rewards[self.step].copy_(rewards)
        self.masks[self.step + 1].copy_(masks)
        self.bad_masks[self.step + 1].copy_(bad_masks)

        self.step = (self.step + 1) % self.num_steps

    def after_update(self):
        self.obs[0].copy_(self.obs[-1])
        self.masks[0].copy_(self.masks[-1])
        self.bad_masks[0].copy_(self.bad_masks[-1])

    def compute_returns(self,
                        next_value,
                        use_gae,
                        gamma,
                        gae_lambda,
                        use_proper_time_limits=True):
        if use_proper_time_limits:
            if use_gae:
                self.value_preds[-1] = next_value
                gae = 0
                for step in reversed(range(self.rewards.size(0))):
                    delta = self.rewards[step] + gamma * self.value_preds[
                        step + 1] * self.masks[step +
                                               1] - self.value_preds[step]
                    gae = delta + gamma * gae_lambda * self.masks[step +
                                                                  1] * gae
                    gae = gae * self.bad_masks[step + 1]
                    self.returns[step] = gae + self.value_preds[step]
            else:
                self.returns[-1] = next_value
                for step in reversed(range(self.rewards.size(0))):
                    self.returns[step] = (self.returns[step + 1] * \
                        gamma * self.masks[step + 1] + self.rewards[step]) * self.bad_masks[step + 1] \
                        + (1 - self.bad_masks[step + 1]) * self.value_preds[step]
        else:
            if use_gae:
                self.value_preds[-1] = next_value
                gae = 0
                for step in reversed(range(self.rewards.size(0))):
                    delta = self.rewards[step] + gamma * self.value_preds[
                        step + 1] * self.masks[step +
                                               1] - self.value_preds[step]
                    gae = delta + gamma * gae_lambda * self.masks[step +
                                                                  1] * gae
                    self.returns[step] = gae + self.value_preds[step]
            else:
                self.returns[-1] = next_value
                for step in reversed(range(self.rewards.size(0))):
                    self.returns[step] = self.returns[step + 1] * \
                        gamma * self.masks[step + 1] + self.rewards[step]

    def feed_forward_generator(self,
                               advantages,
                               num_mini_batch=None,
                               mini_batch_size=None):
        num_steps, num_processes = self.rewards.size()[0:2]
        batch_size = num_processes * num_steps

        if mini_batch_size is None:
            assert batch_size >= num_mini_batch, (
                "PPO requires the number of processes ({}) "
                "* number of steps ({}) = {} "
                "to be greater than or equal to the number of PPO mini batches ({})."
                "".format(num_processes, num_steps, num_processes * num_steps,
                          num_mini_batch))
            mini_batch_size = batch_size // num_mini_batch
        sampler = BatchSampler(
            SubsetRandomSampler(range(batch_size)),
            mini_batch_size,
            drop_last=True)
        for indices in sampler:
            obs_batch = self.obs[:-1].view(-1, *self.obs.size()[2:])[indices]
            actions_batch = self.actions.view(-1,
                                              self.actions.size(-1))[indices]
            value_preds_batch = self.value_preds[:-1].view(-1, 1)[indices]
            return_batch = self.returns[:-1].view(-1, 1)[indices]
            masks_batch = self.masks[:-1].view(-1, 1)[indices]
            old_action_log_probs_batch = self.action_log_probs.view(-1,
                                                                    1)[indices]
            if advantages is None:
                adv_targ = None
            else:
                adv_targ = advantages.view(-1, 1)[indices]

            yield obs_batch, actions_batch, \
                value_preds_batch, return_batch, masks_batch, old_action_log_probs_batch, adv_targ

from gym.wrappers import RecordVideo

def always_true(i_epoch):
    return True

import time

if __name__ == '__main__':
    num_processes = 4  # Quantidade de processos rodando em paralelo
    env = SubprocVecEnv([make_env for _ in range(num_processes)], start_method='spawn')  # Pilha de 'num_processes' 
    env = VecNormalize(env, norm_obs=False, norm_reward=True)  # Normalize observations and rewards
    env.reset()  # Reset all the 'num_processes' environments

    actor_critic = Policy(
        env.observation_space.shape,
        env.action_space,
        activation=None
    )  # Define a política a ser seguida pelo agente
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    actor_critic = actor_critic.to(device)



    clip_param = 0.1
    ppo_epoch = 16
    mini_batch_size = 256
    value_loss_coef = 0.5
    entropy_coef = 0.01
    lr = 2.5e-4
    eps = 1e-5
    gailgamma = None
    decay = None
    max_grad_norm = 0.5

    agent = PPO(
            actor_critic,
            clip_param,
            ppo_epoch,
            mini_batch_size,
            value_loss_coef,
            entropy_coef,
            lr=lr,
            eps=eps,
            gamma=gailgamma,
            decay=decay,
            act_space=env.action_space,
            max_grad_norm=max_grad_norm)


    num_steps = 1024

    rollouts = RolloutStorage(num_steps, num_processes, env.observation_space.shape, env.action_space)

    obs = env.reset()
    obs = torch.from_numpy(obs)
    rollouts.obs.copy_(obs)

    video_path = "./videoo"
    env_video = CarRacing(render_mode="rgb_array", domain_randomize=False)
    env_video = FrameStack(env_video, 4)
    env_video = TransformImage(env_video)
    env_video = TimeLimit(env_video, 1000)
    env_video = TransformAction(env_video)
    env_video = RecordVideo(env_video, video_path, episode_trigger=always_true)

    num_epochs = 200
    ep_reward = 0
    start = time.time()
    for i_epoch in range(num_epochs):
        episode_rewards = []
        for step in range(num_steps):
            # Sample actions
            with torch.no_grad():
                value, action, action_log_prob = actor_critic.act(
                    rollouts.obs[step].to(device))

            action = action.cpu().numpy()

            # Observe reward and next obs
            obs, reward, done, infos = env.step(action)

            for info in infos:
                if 'episode' in info.keys():
                    episode_rewards.append(info['episode']['r'])

            # If done then clean the history of observations.
            masks = torch.FloatTensor(
                [[0.0] if done_ else [1.0] for done_ in done])
            bad_masks = torch.FloatTensor(
                [[0.0] if 'bad_transition' in info.keys() else [1.0]
                for info in infos])

            obs = torch.from_numpy(obs)
            action = torch.from_numpy(action)
            reward = torch.from_numpy(reward).unsqueeze(1)
            # If done then clean the history of observations.
            rollouts.insert(obs, action,
                            action_log_prob, value, reward, masks, bad_masks)
        with torch.no_grad():
            next_value = actor_critic.get_value(
                rollouts.obs[-1].to(device)).detach()

        use_gae = True
        gamma = 0.99
        gae_lambda = 0.95
        use_proper_time_limits = False

        rollouts.compute_returns(next_value, use_gae, gamma,
                                gae_lambda, use_proper_time_limits)

        print("agent.update(rollouts)")
        rollouts.to(device)
        value_loss, action_loss, dist_entropy = agent.update(rollouts)
        print("after agent.update(rollouts)")

        rollouts.after_update()
        print('value_loss:', value_loss)
        print('action_loss', action_loss)
        print('dist_entropy', dist_entropy)
        if len(episode_rewards) > 1:
            total_num_steps = (i_epoch + 1) * num_processes * num_steps
            end = time.time()
            print(
            "Updates {}, num timesteps {}, FPS {} \n Last {} training episodes: mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}\n"
            .format(i_epoch, total_num_steps,
                    int(total_num_steps / (end - start)),
                    len(episode_rewards), np.mean(episode_rewards),
                    np.median(episode_rewards), np.min(episode_rewards),
                    np.max(episode_rewards), dist_entropy, value_loss,
                    action_loss))
        if i_epoch % 8 == 0:
            torch.save(actor_critic.state_dict(), 'model_{}.pt'.format(i_epoch))
            obs, _ = env_video.reset()
            done = False
            step = 0
            while not done:
                obs = torch.from_numpy(obs).float()
                obs = obs.unsqueeze(0).to(device)
                with torch.no_grad():
                    _, action, _ = actor_critic.act(obs, deterministic=True)
                action = action[0].cpu().numpy()
                obs, reward, done, truncated, info = env_video.step(action)
                done |= truncated
                step += 1
